## Cross Feature Configuration

# Using tf.feature_column.crossed_column API,
# see https://www.tensorflow.org/api_docs/python/tf/feature_column/crossed_column
# Transform: Hash(cartesian product of features) % hash_bucket_size

# Q: How to chose `hash_bucket_size` ?
# A: 1. if we know the total number of distinct value `N` of a cross feature,
#       to avoid collision, set `hash_bucket_size` about 2*N.
#    2. if we do not know,
#       1) for small size category cross features, recommended `hash_bucket_size` to be production of each cross feature distinct value count.
#          For example, cross feature {age, location}, age has n values, gender has m values,
#       then hash_bucket_size can be set about n*m.
#       2) for large size category cross features, consider high sparsity,
#          recommended `hash_bucket_size` to be above rule divide 10~1000.

# Conf Format Example
# features:               # cross feature names, at least 2, separated by `&`
#   hash_bucket_size:     # optional, if set empty, defaults 10(k), (unit is a thousand).
#   is_deep:              # `0` or `1`, set `0` for only wide input, set `1` for embedding the cross feature for deep input
                          # if set empty, defaults to `1`

# Set unused features by using symbol `#` ahead of the lines.

# 31 in total (all hash), the number is multiplied by 1000
# 100 * 1 + 1000 * 5 + 3000 * 1 + 10000 * 12 + 30000 * 1 + 100000 * 10 + 1000000 * 1

# Small: 100 * 1 + 1000 * 3
# age&ugender:
#   hash_bucket_size: 0.1
#   is_deep: 1

# site&age&ugender:
#   hash_bucket_size: 1
#   is_deep: 1

# hour&weekdays:
#   hash_bucket_size: 1
#   is_deep: 1

# network&os&site:
#   hash_bucket_size: 1
#   is_deep: 1


# Left for cross-feature

# HBM -> 12 * 50000 + 11 * 100000 + 2 * 150000 + 2 * 250000 + 16 * 500000 + 2 * 1000000

# data_size_DDR =  [12, 16, 32, 32]
# table_size_DDR = [5000000, 5000000, 10000000, 100000000]


age0&ugender0:
  hash_bucket_size: 50
  is_deep: 1

site0&age0&ugender0:
  hash_bucket_size: 50
  is_deep: 1

hour0&weekdays0:
  hash_bucket_size: 50
  is_deep: 1

network0&os0&site0:
  hash_bucket_size: 50
  is_deep: 1


age1&ugender1:
  hash_bucket_size: 50
  is_deep: 1

site1&age1&ugender1:
  hash_bucket_size: 50
  is_deep: 1

hour1&weekdays1:
  hash_bucket_size: 50
  is_deep: 1

network1&os1&site1:
  hash_bucket_size: 50
  is_deep: 1


age2&ugender2:
  hash_bucket_size: 50
  is_deep: 1

site2&age2&ugender2:
  hash_bucket_size: 50
  is_deep: 1

hour2&weekdays2:
  hash_bucket_size: 50
  is_deep: 1

network2&os2&site2:
  hash_bucket_size: 50
  is_deep: 1

#### 12 * 50K finished

# Medium: 1000 * 2 + 3000 * 1 + 10000 * 12 + 30000 * 1 + 100000 * 8
# industry_level2_id&site:
#   hash_bucket_size: 1
#   is_deep: 1

# idea_type&os&network:
#   hash_bucket_size: 1
#   is_deep: 1

# industry_level2_id&age&ugender:
#   hash_bucket_size: 3
#   is_deep: 1

# industry_level2_id&network&os:
#   hash_bucket_size: 10
#   is_deep: 1

# city_id&industry_level2_id:
#   hash_bucket_size: 10
#   is_deep: 1

# idea_type&city_id:
#   hash_bucket_size: 10
#   is_deep: 1

# idea_id&province_id:
#   hash_bucket_size: 10
#   is_deep: 1

# idea_id&device_type&os:
#   hash_bucket_size: 10
#   is_deep: 1

# hour&scheduling_id:
#   hash_bucket_size: 10
#   is_deep: 1

# hour&industry_level2_id:
#   hash_bucket_size: 10
#   is_deep: 1

# scheduling_id&age:
#   hash_bucket_size: 10
#   is_deep: 1

# category&ucomp:
#   hash_bucket_size: 10
#   is_deep: 1

# industry_level2_id&ucomp:
#   hash_bucket_size: 10
#   is_deep: 1

# category&industry_level2_id:
#   hash_bucket_size: 10
#   is_deep: 1

# category&network&os:
#   hash_bucket_size: 10
#   is_deep: 1

# industry_level2_id&user_industrys:
#   hash_bucket_size: 30
#   is_deep: 1

# adplan_id&category:
#   hash_bucket_size: 100
#   is_deep: 1

# scheduling_id&ucomp:
#   hash_bucket_size: 100
#   is_deep: 1

# category&location&site:
#   hash_bucket_size: 100
#   is_deep: 1

# scheduling_id&age&ugender:
#   hash_bucket_size: 100
#   is_deep: 1

# adplan_id&ucomp:
#   hash_bucket_size: 100
#   is_deep: 1

# category&scheduling_id:
#   hash_bucket_size: 100
#   is_deep: 1

# adplan_id&os&site:
#   hash_bucket_size: 100
#   is_deep: 1

# network&os&scheduling_id:
#   hash_bucket_size: 100
#   is_deep: 1
industry_level2_id0&site0:
  hash_bucket_size: 100
  is_deep: 1

idea_type0&os0&network0:
  hash_bucket_size: 100
  is_deep: 1

industry_level2_id0&age0&ugender0:
  hash_bucket_size: 100
  is_deep: 1

industry_level2_id0&network0&os0:
  hash_bucket_size: 100
  is_deep: 1

city_id0&industry_level2_id0:
  hash_bucket_size: 100
  is_deep: 1

idea_type0&city_id0:
  hash_bucket_size: 100
  is_deep: 1

idea_id0&province_id0:
  hash_bucket_size: 100
  is_deep: 1

idea_id0&device_type0&os0:
  hash_bucket_size: 100
  is_deep: 1

hour0&scheduling_id0:
  hash_bucket_size: 100
  is_deep: 1

hour0&industry_level2_id0:
  hash_bucket_size: 100
  is_deep: 1

scheduling_id0&age0:
  hash_bucket_size: 100
  is_deep: 1

# 11 x 100K finished

category0&ucomp0:
  hash_bucket_size: 150
  is_deep: 1

industry_level2_id0&ucomp0:
  hash_bucket_size: 150
  is_deep: 1

category0&industry_level2_id0:
  hash_bucket_size: 250
  is_deep: 1

category0&network0&os0:
  hash_bucket_size: 250
  is_deep: 1

# 2 x 150K + 2 x 250 K finished

industry_level2_id0&user_industrys0:
  hash_bucket_size: 500
  is_deep: 1

adplan_id0&category0:
  hash_bucket_size: 500
  is_deep: 1

scheduling_id0&ucomp0:
  hash_bucket_size: 500
  is_deep: 1

category0&location0&site0:
  hash_bucket_size: 500
  is_deep: 1

scheduling_id0&age0&ugender0:
  hash_bucket_size: 500
  is_deep: 1

adplan_id0&ucomp0:
  hash_bucket_size: 500
  is_deep: 1

category0&scheduling_id0:
  hash_bucket_size: 500
  is_deep: 1

adplan_id0&os0&site0:
  hash_bucket_size: 500
  is_deep: 1

network0&os0&scheduling_id0:
  hash_bucket_size: 500
  is_deep: 1


industry_level2_id1&site1:
  hash_bucket_size: 500
  is_deep: 1

idea_type1&os1&network1:
  hash_bucket_size: 500
  is_deep: 1

industry_level2_id1&age1&ugender1:
  hash_bucket_size: 500
  is_deep: 1

industry_level2_id1&network1&os1:
  hash_bucket_size: 500
  is_deep: 1

city_id1&industry_level2_id1:
  hash_bucket_size: 500
  is_deep: 1

idea_type1&city_id1:
  hash_bucket_size: 500
  is_deep: 1

idea_id1&province_id1:
  hash_bucket_size: 500
  is_deep: 1

# 16 x 500 K finished

idea_id1&device_type1&os1:
  hash_bucket_size: 1000
  is_deep: 1

hour1&scheduling_id1:
  hash_bucket_size: 1000
  is_deep: 1

# 2 x 1000 K finished 

# table_size_DDR = [5000000, 5000000, 10000000, 100000000]
hour1&industry_level2_id1:
  hash_bucket_size: 5000
  is_deep: 1

# scheduling_id1&age1:
#   hash_bucket_size: 50
#   is_deep: 1

# category1&ucomp1:
#   hash_bucket_size: 50
#   is_deep: 1

# industry_level2_id1&ucomp1:
#   hash_bucket_size: 50
#   is_deep: 1

# category1&industry_level2_id1:
#   hash_bucket_size: 50
#   is_deep: 1

# category1&network1&os1:
#   hash_bucket_size: 50
#   is_deep: 1

# industry_level2_id1&user_industrys1:
#   hash_bucket_size: 150
#   is_deep: 1

# adplan_id1&category1:
#   hash_bucket_size: 500
#   is_deep: 1

# scheduling_id1&ucomp1:
#   hash_bucket_size: 500
#   is_deep: 1

# category1&location1&site1:
#   hash_bucket_size: 500
#   is_deep: 1

# scheduling_id1&age1&ugender1:
#   hash_bucket_size: 500
#   is_deep: 1

# adplan_id1&ucomp1:
#   hash_bucket_size: 500
#   is_deep: 1

# category1&scheduling_id1:
#   hash_bucket_size: 500
#   is_deep: 1

# adplan_id1&os1&site1:
#   hash_bucket_size: 500
#   is_deep: 1

# network1&os1&scheduling_id1:
#   hash_bucket_size: 500
#   is_deep: 1




# Large: 100000 * 2 + 1000000 * 1

# industry_level2_id&ip_original:
#   hash_bucket_size: 100
#   is_deep: 1

# idea_id&site&category:
#   hash_bucket_size: 100
#   is_deep: 1

# adplan_id&category&ucomp:
#   hash_bucket_size: 1000
#   is_deep: 1

industry_level2_id0&ip_original0:
  hash_bucket_size: 5000
  is_deep: 1

idea_id0&site0&category0:
  hash_bucket_size: 10000
  is_deep: 1

adplan_id0&category0&ucomp0:
  hash_bucket_size: 100000
  is_deep: 1

